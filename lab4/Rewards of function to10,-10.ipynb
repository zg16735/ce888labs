{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## imports \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define a generic bandit class\n",
    "class Bandit(object): \n",
    "    def __init__(self,n_actions): \n",
    "        self.counts = np.zeros(n_actions) \n",
    "        self.action_rewards = [[] for i in range(n_actions)] \n",
    "        self.rewards = [] \n",
    "        self.n_actions = n_actions\n",
    "    def select_action(self): \n",
    "        \"\"\"Selection which arm/action to pull\"\"\" \n",
    "        pass\n",
    "    def update(self,action,reward): \n",
    "        \"\"\"Update the actions\"\"\" \n",
    "        self.counts[action] = self.counts[action] + 1 \n",
    "        self.action_rewards[action].append(reward) \n",
    "        self.rewards.append(reward)\n",
    "    def get_Q_values(self): \n",
    "        Q_values = [] \n",
    "        for q_v in self.action_rewards: \n",
    "            Q_values.append(np.array(q_v).mean())\n",
    "        return np.array(Q_values)\n",
    "    def get_V_value(self): \n",
    "        return np.array(self.v_value.mean())\n",
    "    \n",
    "n_trials = 15000\n",
    "t_max = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## And three actions\n",
    "\n",
    "n_actions = 3\n",
    "\n",
    "def action_0():\n",
    "    return np.random.choice([10,-10], p=[0.5, 0.5])\n",
    "\n",
    "def action_1():\n",
    "    return np.random.choice([10,-10], p=[0.6, 0.4])\n",
    "\n",
    "def action_2():\n",
    "    return np.random.choice([10,-10], p=[0.2, 0.8])\n",
    "\n",
    "rewards = [action_0, action_1, action_2]\n",
    "\n",
    "\n",
    "## E-greedy\n",
    "\n",
    "\n",
    "class Egreedy(Bandit):\n",
    "    def __init__(self, epsilon,  *args,  **kwargs):\n",
    "        super(Egreedy, self).__init__(*args, **kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def select_action(self):\n",
    "        \n",
    "       \n",
    "        \n",
    "        #never_visited = np.nonzero(self.counts == 0)\n",
    "#         print indices_zero, \"dfdf\"\n",
    "        never_visited = np.where(self.counts == 0)[0]\n",
    "        #print never_visited\n",
    "        if(len(never_visited)!=0):\n",
    "            return np.random.choice(never_visited)\n",
    "        \n",
    "        \n",
    "        #print never_visited, \"never\"\n",
    "        Q_values = self.get_Q_values()\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return np.argmax(Q_values)\n",
    "        else:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "\n",
    "class UCB(Bandit):\n",
    "    def __init__(self,  *args,  **kwargs):\n",
    "        super(UCB, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        \n",
    "    def select_action(self):\n",
    "        \n",
    "       \n",
    "        never_visited = np.where(self.counts == 0)[0]\n",
    "        #print never_visited\n",
    "        if(len(never_visited)!=0):\n",
    "            return np.random.choice(never_visited)\n",
    "        \n",
    "        #print self.counts\n",
    "        #print never_visited, \"never\"\n",
    "        Q_values = self.get_Q_values()\n",
    "        #UCB = Q_values + np.sqrt(2 * np.log(len(self.v_values) + 1) / numPlays)\n",
    "        #print self.counts\n",
    "        for i in range(0,len(Q_values)):\n",
    "            #print np.sqrt((2 * np.log(len(self.v_values) + 1)) / len(self.q_values[i]))\n",
    "            Q_values[i]+= 0.5 *  np.sqrt(( np.log(len(self.rewards))) / len(self.action_rewards[i]))\n",
    "        return np.argmax(Q_values)\n",
    "\n",
    "    \n",
    "class BootstrapThompson(Bandit):\n",
    "    def __init__(self, *args,  **kwargs):\n",
    "        super(BootstrapThompson, self).__init__(*args, **kwargs)\n",
    "        #self.heads = [[[] for range(self.n_actions)] for range(n_heads)]\n",
    "        #self.n_heads = n_heads\n",
    "        \n",
    "        \n",
    "    def select_action(self):\n",
    "        \n",
    "       \n",
    "        never_visited = np.where(self.counts < 5)[0]\n",
    "        #print never_visited\n",
    "        if(len(never_visited)!=0):\n",
    "            return np.random.choice(never_visited)\n",
    "        \n",
    "        Q_values = []\n",
    "        for i,q_v in enumerate(self.action_rewards):\n",
    "            b_sample = np.random.choice(q_v, len(q_v), replace=True)\n",
    "            Q_values.append(b_sample.mean())\n",
    "            #print i, q_v, \"qv\"\n",
    "        #print Q_values\n",
    "        #print (\"===========\")\n",
    "        return np.array(Q_values).argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "bandits = [(\"0.2-decreasing_10\",  lambda: Egreedy(epsilon = 0.2, n_actions = n_actions)),\n",
    "           (\"UCB_10\", lambda: UCB( n_actions = n_actions)),\n",
    "           (\"BootstrapTS_10\",  lambda: BootstrapThompson( n_actions = n_actions)),\n",
    "          \n",
    "          ]\n",
    "\n",
    "for b in bandits:\n",
    "\n",
    "    columns =[\"Step\", \"Cumulative Regret\", \"trial\", \"Algorithm\"]\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "   \n",
    "    for trial in (range(n_trials)):\n",
    "        gaps = []\n",
    "        bandit = b[1]()\n",
    "       \n",
    "        for i in range(t_max):\n",
    "            # select action\n",
    "            action = bandit.select_action()\n",
    "            # get the reward\n",
    "            reward = rewards[action]()\n",
    "            # update the \n",
    "            bandit.update(action, reward)\n",
    "            # Super hack for the lazy\n",
    "            if(b[0].endswith(\"decreasing_10\")):\n",
    "                #print \"decreasing\"\n",
    "                bandit.epsilon*=0.99\n",
    "               \n",
    "            gaps.append(0.6 - reward)\n",
    "            regret = np.array(gaps).sum()\n",
    "            data.append([i,regret,trial, b[0]])\n",
    "            #print df.head()\n",
    "        #print trial\n",
    "    df = df.append(data)\n",
    "    df.columns = columns\n",
    "    df.head()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    axes = sns.tsplot(time=\"Step\", value=\"Cumulative Regret\",\n",
    "                     unit=\"trial\", condition=\"Algorithm\",data=df)\n",
    "\n",
    "\n",
    "    axes.set_ylim([0,40])\n",
    "\n",
    "    plt.savefig(b[0] + \".pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
